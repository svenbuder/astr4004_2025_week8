{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5366df",
   "metadata": {},
   "source": [
    "# ANU ASTR4004 2025 - Week 8 (23+25 September 2025)\n",
    "\n",
    "Author: Dr Sven Buder (sven.buder@anu.edu.au)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a203675",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Building-intuition:-Ordinary-Least-Squares\" data-toc-modified-id=\"Building-intuition:-Ordinary-Least-Squares-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Building intuition: Ordinary Least Squares</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-based-searches\" data-toc-modified-id=\"Grid-based-searches-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Grid-based searches</a></span></li><li><span><a href=\"#Minimizing-the-error-function\" data-toc-modified-id=\"Minimizing-the-error-function-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Minimizing the error function</a></span></li></ul></li><li><span><a href=\"#Fitting-a-model-to-data\" data-toc-modified-id=\"Fitting-a-model-to-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Fitting a model to data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Complexity-0:-The-simplest-case\" data-toc-modified-id=\"Complexity-0:-The-simplest-case-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Complexity 0: The simplest case</a></span></li><li><span><a href=\"#Complexity-1:-uncertainty-in-y\" data-toc-modified-id=\"Complexity-1:-uncertainty-in-y-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Complexity 1: uncertainty in y</a></span></li><li><span><a href=\"#Complexity-2:-Outliers\" data-toc-modified-id=\"Complexity-2:-Outliers-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Complexity 2: Outliers</a></span></li><li><span><a href=\"#Complexity-3+:-A-quadratic-function-with-a-slightly-different-naming-convention\" data-toc-modified-id=\"Complexity-3+:-A-quadratic-function-with-a-slightly-different-naming-convention-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Complexity 3+: A quadratic function with a slightly different naming convention</a></span></li></ul></li><li><span><a href=\"#When-should-I-use-what-python-fitting-function?\" data-toc-modified-id=\"When-should-I-use-what-python-fitting-function?-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>When should I use what python fitting function?</a></span><ul class=\"toc-item\"><li><span><a href=\"#numpy.polyfit\" data-toc-modified-id=\"numpy.polyfit-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>numpy.polyfit</a></span></li><li><span><a href=\"#scipy.optimize.curve_fit\" data-toc-modified-id=\"scipy.optimize.curve_fit-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>scipy.optimize.curve_fit</a></span></li><li><span><a href=\"#statsmodels.api-as-sm\" data-toc-modified-id=\"statsmodels.api-as-sm-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>statsmodels.api as sm</a></span></li></ul></li><li><span><a href=\"#What-if-I-also-have-uncertainties-on-my-x-data?\" data-toc-modified-id=\"What-if-I-also-have-uncertainties-on-my-x-data?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>What if I also have uncertainties on my x-data?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Methodology\" data-toc-modified-id=\"Methodology-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Methodology</a></span><ul class=\"toc-item\"><li><span><a href=\"#Total-Uncertainty-($\\sigma_{\\text{total}}$)-in-$y_i$\" data-toc-modified-id=\"Total-Uncertainty-($\\sigma_{\\text{total}}$)-in-$y_i$-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Total Uncertainty ($\\sigma_{\\text{total}}$) in $y_i$</a></span></li><li><span><a href=\"#Propagating-the-Uncertainty-in-$x_i$\" data-toc-modified-id=\"Propagating-the-Uncertainty-in-$x_i$-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Propagating the Uncertainty in $x_i$</a></span></li><li><span><a href=\"#Total-Uncertainty-in-$y_i$\" data-toc-modified-id=\"Total-Uncertainty-in-$y_i$-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Total Uncertainty in $y_i$</a></span></li><li><span><a href=\"#Likelihood-Function\" data-toc-modified-id=\"Likelihood-Function-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Likelihood Function</a></span></li><li><span><a href=\"#Log-Likelihood\" data-toc-modified-id=\"Log-Likelihood-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>Log-Likelihood</a></span></li></ul></li><li><span><a href=\"#Maximum-Likelihood-Estimation-(MLE)\" data-toc-modified-id=\"Maximum-Likelihood-Estimation-(MLE)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Maximum Likelihood Estimation (MLE)</a></span></li><li><span><a href=\"#scipy.odr:-Orthogonal-Distance-Regression\" data-toc-modified-id=\"scipy.odr:-Orthogonal-Distance-Regression-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>scipy.odr: Orthogonal Distance Regression</a></span></li><li><span><a href=\"#Sneak-Preview:-Bayesian-Fitting-with-a-flat-prior\" data-toc-modified-id=\"Sneak-Preview:-Bayesian-Fitting-with-a-flat-prior-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Sneak Preview: Bayesian Fitting with a flat prior</a></span></li></ul></li><li><span><a href=\"#Jackknife-and-Bootstrap-Resampling\" data-toc-modified-id=\"Jackknife-and-Bootstrap-Resampling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Jackknife and Bootstrap Resampling</a></span></li><li><span><a href=\"#A-closer-look-at-uncertainties-in-likelihood-functions\" data-toc-modified-id=\"A-closer-look-at-uncertainties-in-likelihood-functions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>A closer look at uncertainties in likelihood functions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44ccb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import patches\n",
    "\n",
    "# Make the size and fonts larger for this presentation\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['legend.fontsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a4ddc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def round_to_significant_digits(value, uncertainty, sig_figs=2):\n",
    "    \"\"\"\n",
    "    Rounds the value and uncertainty to the same number of significant decimal places\n",
    "    based on the uncertainty, allowing control of the significant digits. If the uncertainty \n",
    "    is larger than 10, rounds to the nearest integer.\n",
    "    \n",
    "    Parameters:\n",
    "    value (float): The measurement value.\n",
    "    uncertainty (float): The measurement uncertainty.\n",
    "    sig_figs (int): The number of significant figures to round the uncertainty to.\n",
    "                    Default is 2 (i.e., the second significant figure of the uncertainty).\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (rounded_value, rounded_uncertainty) with appropriate significant digits or integers.\n",
    "    \"\"\"\n",
    "    if uncertainty == 0:\n",
    "        return value, uncertainty\n",
    "    \n",
    "    # Check if the uncertainty is larger than 10, if so round to nearest integer\n",
    "    if uncertainty >= 10:\n",
    "        rounded_value = round(value)\n",
    "        rounded_uncertainty = round(uncertainty)\n",
    "    else:\n",
    "        # Find the first non-zero digit of the uncertainty\n",
    "        decimal_places = -int(np.floor(np.log10(abs(uncertainty)))) + (sig_figs - 1)\n",
    "    \n",
    "        # Round the value and uncertainty to the same number of decimal places\n",
    "        rounded_value = round(value, decimal_places)\n",
    "        rounded_uncertainty = round(uncertainty, decimal_places)\n",
    "    \n",
    "    return rounded_value, rounded_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871238d",
   "metadata": {},
   "source": [
    "## Building intuition: Ordinary Least Squares\n",
    "\n",
    "The simple regression model estimates the relationship between two variables $x_i$ and $y_i$\n",
    "\n",
    "$$\n",
    "y_i = c_0 + c_1 \\cdot x_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i$ represents the error between the line of best fit and the sample values for $y_i$ given $x_i$. Our model here is\n",
    "\n",
    "$$\n",
    "\\hat{y} = c_0 + c_1 \\cdot x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92660404",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Example taken from https://intro.quantecon.org/simple_linear_regression.html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "x = [32, 21, 24, 35, 10, 11, 22, 21, 27, 2]\n",
    "y = [2000,1000,1500,2500,500,900,1100,1500,1800, 250]\n",
    "df = pd.DataFrame([x,y]).T\n",
    "df.columns = ['X', 'Y']\n",
    "\n",
    "ax = df.plot(\n",
    "    x='X', \n",
    "    y='Y', \n",
    "    kind='scatter', \n",
    "    ylabel='Ice-cream sales ($\\'s)', \n",
    "    xlabel='Degrees celcius'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6070835",
   "metadata": {},
   "source": [
    "We can do a bit of \"chi by eye\" fitting and find two coefficients, calculate the model $\\hat{y}$.\n",
    "We can also estimate the model error $\\hat{e}_i$ at each point:\n",
    "\n",
    "$$\n",
    "\\hat{e}_i = y_i - \\hat{y}_i = y_i - (c_0 + c_1 \\cdot x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the error function (sum of squared differences)\n",
    "def model_function(c_0, c_1, x):\n",
    "    y_model = c_0 + c_1 * x\n",
    "    return y_model\n",
    "\n",
    "def error_function(c_0, c_1, x, y):\n",
    "    y_model = model_function(c_0, c_1, x)\n",
    "    return np.sum((y_model - y) ** 2)\n",
    "\n",
    "c_0 = 5\n",
    "c_1 = 60\n",
    "\n",
    "df['Y_hat'] = model_function(c_0, c_1, df['X'])\n",
    "df['error'] = error_function(c_0, c_1, df['X'], df['Y'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax = df.plot(x='X',y='Y', kind='scatter', ax=ax, c = 'k', label = r'Measurement $y_i$')\n",
    "ax = df.plot(x='X',y='Y_hat', kind='line', ax=ax, color='C1', lw = 1, label = r'Model $\\hat{y}$')\n",
    "plt.vlines(df['X'], df['Y_hat'], df['Y'], color='C3', label =r'Residual $\\hat{e}_i$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711a5c76",
   "metadata": {},
   "source": [
    "The Ordinary Least Squares (OLS) method chooses $c_0$ and $c_1$ in such a way that **minimizes** the sum of the squared residuals (SSR):\n",
    "\n",
    "$$\n",
    "\\min_{c_0,c_1} \\sum_{i=1}^N \\hat{e}_i^2 = \\min_{c_0,c_1} \\sum_{i=1}^N (y_i - (c_0 + c_1 \\cdot x_i))^2 = \\min_{c_0,c_1} E\n",
    "$$\n",
    "\n",
    "There are multiple ways how we could solve this.  \n",
    "One of them could certainly be to select a range of $c_0$ and/or $c_1$ values calculate the error function.  \n",
    "Let's first do test the behavior of the error function when fixing $c_0$ to 0 and changing $c_1$.  \n",
    "In a second step, let's optimise over a grid of changing $c_0$ and $c_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadd3cf",
   "metadata": {},
   "source": [
    "### Grid-based searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5833c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1_values = np.linspace(25, 100, 10)\n",
    "errors_c1 = [error_function(0, c_1, df['X'], df['Y']) for c_1 in c_1_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30448ff3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot error function vs c_1 for c_0 = 0\n",
    "plt.figure()\n",
    "plt.plot(c_1_values, errors_c1)\n",
    "plt.xlabel('c_1')\n",
    "plt.ylabel('E')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a grid of c_0 and c_1 and calculate the error\n",
    "\n",
    "c_0_range = np.linspace(-500, 500, 11) # -500..(100)..500\n",
    "c_1_range = np.linspace(0, 100, 11) # 0..(10)..100\n",
    "\n",
    "# create a (10,10) mesh grid from c_0 and c_1\n",
    "c_0_vals, c_1_vals = np.meshgrid(c_0_range, c_1_range)\n",
    "print('Shape of c_0_vals and c_1_vals: ',np.shape(c_0_vals), np.shape(c_0_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13565f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "errors_grid = np.zeros((len(c_0_range), len(c_1_range)))\n",
    "for i, c_0 in enumerate(c_0_range):\n",
    "    for j, c_1 in enumerate(c_1_range):\n",
    "        errors_grid[i, j] = error_function(c_0, c_1, df['X'], df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b43c8f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the error as a heatmap and scatter plot\n",
    "f, gs = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax = gs[0]\n",
    "s = ax.contourf(c_0_range, c_1_range, errors_grid, cmap='RdYlBu')\n",
    "plt.colorbar(s, ax=ax,label='E')\n",
    "ax.set_xlabel('c_0')\n",
    "ax.set_ylabel('c_1')\n",
    "ax.set_title('Heatmap')\n",
    "\n",
    "ax = gs[1]\n",
    "s = plt.scatter(c_0_vals, c_1_vals, c = errors_grid, label='Grid Points', cmap='RdYlBu')\n",
    "plt.colorbar(s, ax=ax,label='E')\n",
    "ax.set_xlabel('c_0')\n",
    "ax.set_ylabel('c_1')\n",
    "ax.set_title('Scatter points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b469dbb",
   "metadata": {},
   "source": [
    "Clearly the range that we cover is covering a significant range in error\n",
    "\n",
    "This is maybe not the best visualisation. Let's plot the logarithmic values of the error function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368f366",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the ln(error) as a heatmap and scatter\n",
    "f, gs = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax = gs[0]\n",
    "s = ax.contourf(c_0_range, c_1_range, np.log(errors_grid), cmap='RdYlBu')\n",
    "plt.colorbar(s, ax=ax,label='ln(E)')\n",
    "ax.set_xlabel('c_0')\n",
    "ax.set_ylabel('c_1')\n",
    "ax.set_title('Heatmap')\n",
    "\n",
    "ax = gs[1]\n",
    "s = plt.scatter(c_0_vals, c_1_vals, c = np.log(errors_grid), label='Grid Points', cmap='RdYlBu')\n",
    "plt.colorbar(s, ax=ax,label='ln(E)')\n",
    "ax.set_xlabel('c_0')\n",
    "ax.set_ylabel('c_1')\n",
    "ax.set_title('Scatter points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcfe57",
   "metadata": {},
   "source": [
    "This way of estimating optimal parameters is a grid-based search. There are reasons to use it (e.g. if you want to avoid getting stuck in local minima).\n",
    "\n",
    "But you might want to consider smarter methods, if you can - or if you do not know which range you actually have to cover!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb15c8",
   "metadata": {},
   "source": [
    "### Minimizing the error function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64432301",
   "metadata": {},
   "source": [
    "To find the best values for c_0 and c_1, we just need to find the derivatives (using the chain rule)\n",
    "\n",
    "$$\n",
    "0 = \\frac{\\delta E}{\\delta c_0} = \\frac{\\delta}{\\delta c_0} \\sum_{i=1}^N (y_i - (c_0 + c_1 * x_i))^2 = \\sum_{i=1}^N - 2 (y_i - (c_0 + c_1 * x_i)) \\\\\n",
    "0 = \\frac{\\delta E}{\\delta c_1} = \\frac{\\delta}{\\delta c_1} \\sum_{i=1}^N (y_i - (c_0 + c_1 * x_i))^2 = \\sum_{i=1}^N - 2 x_i \\cdot (y_i - (c_0 + c_1 * x_i))\n",
    "$$\n",
    "\n",
    "Dividing by -2 yields\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^N  (y_i - (c_0 + c_1 * x_i)) = \\sum_{i=1}^N y_i - N \\cdot c_0 + c_1 \\cdot \\sum_{i=1}^N x_i \\quad \\rightarrow \\quad c_0 = \\frac{1}{N} \\sum_{i=1}^N y_i - c_1 \\cdot \\frac{1}{N} \\sum_{i=1}^N x_i = \\bar{y_i} - c_1 \\cdot \\bar{x_i}\n",
    "$$\n",
    "\n",
    "We can subsitute this into the partial derivative $\\frac{\\delta E}{\\delta c_1}$ and get\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^N (x_i y_i - \\bar{y_i} x_i) + c_1 \\sum_{i=1}^N (\\bar{x_i} x_i - x_i^2) \\quad \\rightarrow \\quad c_1 = \\frac{\\sum_{i=1}^N (x_i y_i - \\bar{y_i} x_i)}{\\sum_{i=1}^N (x_i^2 - \\bar{x_i} x_i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original data\n",
    "df = df[['X','Y']].copy()\n",
    "\n",
    "# Calculate the sample means\n",
    "x_bar = df['X'].mean()\n",
    "y_bar = df['Y'].mean()\n",
    "\n",
    "# Compute the Sums\n",
    "df['num'] = df['X'] * df['Y'] - y_bar * df['X']\n",
    "df['den'] = df['X']**2 - x_bar * df['X']\n",
    "\n",
    "# Calculate c_1 and then c_0 using the equations above\n",
    "c_1 = df['num'].sum() / df['den'].sum()\n",
    "c_0 = y_bar - c_1 * x_bar\n",
    "\n",
    "print('c_0:', np.round(c_0,2), 'c_1:', np.round(c_1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5e13d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the error as a heatmap  \n",
    "f, gs = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax = gs[0]\n",
    "s = ax.contourf(c_0_range, c_1_range, np.log(errors_grid), cmap='RdYlBu')\n",
    "plt.colorbar(s, ax=ax,label='ln(E)')\n",
    "ax.set_xlabel('c_0')\n",
    "ax.set_ylabel('c_1')\n",
    "ax.set_title('Heatmap')\n",
    "ax.scatter([c_0],[c_1], c = 'k', label = 'OLS', marker='+', s=100)\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "ax = gs[1]\n",
    "s = plt.scatter(c_0_vals, c_1_vals, c = np.log(errors_grid), cmap='RdYlBu', label='Grid Points')\n",
    "plt.colorbar(s, ax=ax,label='ln(E)')\n",
    "ax.set_xlabel('c_0')\n",
    "ax.set_ylabel('c_1')\n",
    "ax.set_title('Scatter points')\n",
    "\n",
    "ax.scatter([c_0],[c_1], c = 'k', label = 'OLS', marker='+', s=100)\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3f6b2",
   "metadata": {},
   "source": [
    "## Fitting a model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba58ce5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Table 1: The Data from Hogg, Bovy, and Lang (2010): https://arxiv.org/abs/1008.4686 \n",
    "\n",
    "x_data_full   = np.array([201, 244, 47, 287, 203, 58, 210, 202, 198, 158, \n",
    "                   165, 201, 157, 131, 166, 160, 186, 125, 218, 146])\n",
    "y_data_full   = np.array([592, 401, 583, 402, 495, 173, 479, 504, 510, 416, \n",
    "                   393, 442, 317, 311, 400, 337, 423, 334, 533, 344])\n",
    "x_sigma_full = np.array([10, 15, 18, 15, 11, 15, 17, 14, 20, 6, \n",
    "                     14, 25, 12, 16, 14, 21, 22, 16, 26, 12])\n",
    "y_sigma_full = np.array([61, 25, 38, 15, 21, 15, 27, 14, 30, 16, \n",
    "                     14, 25, 52, 16, 34, 31, 42, 26, 16, 22])\n",
    "\n",
    "x_data_simple = x_data_full[5:]\n",
    "y_data_simple = y_data_full[5:]\n",
    "y_sigma_simple = y_sigma_full[5:]\n",
    "\n",
    "f, gs = plt.subplots(1,3,sharex=True,sharey=True,figsize=(15,4))\n",
    "\n",
    "ax = gs[0]\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.errorbar(x_data_simple, y_data_simple, yerr=y_sigma_simple, fmt='o', label='Simplest Data Set', c = 'C1')\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "\n",
    "ax = gs[1]\n",
    "ax.set_xlabel('x')\n",
    "ax.errorbar(x_data_full, y_data_full, yerr=y_sigma_full, fmt='o', label='Data Set with outliers', c = 'C3')\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "\n",
    "ax = gs[2]\n",
    "ax.set_xlabel('x')\n",
    "ax.errorbar(x_data_full, y_data_full, xerr = x_sigma_full, yerr=y_sigma_full, fmt='o', lw=1, label='Full Data Set', c = 'k')\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63cfd7",
   "metadata": {},
   "source": [
    "### Complexity 0: The simplest case\n",
    "\n",
    "Let's assume a linear model \n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "- No uncertainties for $x$\n",
    "- No uncertainties for $y$\n",
    "- No outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the data\n",
    "x = x_data_simple\n",
    "y = y_data_simple\n",
    "\n",
    "# Step 2: Create the design matrix X\n",
    "# We add a column of ones for the bias term (c_0)\n",
    "X = np.column_stack((np.ones(x.shape[0]), x))\n",
    "\n",
    "# Step 3: Compute the normal equation components\n",
    "# X^T X and X^T y\n",
    "XT_X = np.dot(X.T, X)  # X.T is the transpose of X\n",
    "XT_y = np.dot(X.T, y)\n",
    "\n",
    "# Step 4: Compute the covariance matrix of the coefficients\n",
    "cov_matrix = np.linalg.inv(XT_X)\n",
    "\n",
    "# Step 5: Solve for the coefficients (c_0, c_1)\n",
    "coefficients = cov_matrix.dot(XT_y)\n",
    "\n",
    "# Step 5: Output the coefficients\n",
    "c_0, c_1 = coefficients\n",
    "\n",
    "# Step 6: Exctract coefficient sigma\n",
    "diagonal_entries_sigma = np.sqrt(np.diag(cov_matrix))\n",
    "c_0_sigma = diagonal_entries_sigma[0]\n",
    "c_1_sigma = diagonal_entries_sigma[1]\n",
    "\n",
    "# Step 7: Use the coefficients to predict y values\n",
    "y_pred = X.dot(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f4453",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot \n",
    "f, ax = plt.subplots()\n",
    "ax.scatter(x, y, c = 'k')\n",
    "ax.plot(x, y_pred, label = f\"y = ({c_0:.2f} ± {c_0_sigma:.2f}) + ({c_1:.2f} ± {c_1_sigma:.2f}) x\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63de590",
   "metadata": {},
   "source": [
    "### Complexity 1: uncertainty in y\n",
    "\n",
    "- No uncertainties for $x$\n",
    "- **Gaussian uncertainties for $y$**\n",
    "- No outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ff01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the data and uncertainties\n",
    "x = x_data_simple\n",
    "y = y_data_simple\n",
    "y_sigma = y_sigma_simple\n",
    "\n",
    "# Step 2: Create the design matrix X\n",
    "# We add a column of ones for the bias term (c_0)\n",
    "X = np.column_stack((np.ones(x.shape[0]), x))\n",
    "\n",
    "# Step 3: Create the weights matrix W\n",
    "W = np.diag(1 / y_sigma**2)  # Diagonal matrix of 1/y_sigma^2\n",
    "\n",
    "# Step 4: Compute the weighted normal equation components\n",
    "# X^T W X and X^T W y\n",
    "XT_W_X = np.dot(X.T, np.dot(W, X))\n",
    "XT_W_y = np.dot(X.T, np.dot(W, y))\n",
    "\n",
    "# Step 5: Compute the covariance matrix of the coefficients\n",
    "cov_matrix = np.linalg.inv(XT_W_X)\n",
    "\n",
    "# Step 5: Solve for the coefficients (c_0, c_1)\n",
    "coefficients = cov_matrix.dot(XT_W_y)\n",
    "\n",
    "# Step 6: Output the coefficients\n",
    "c_0, c_1 = coefficients\n",
    "\n",
    "# Step 7: Exctract coefficient sigma\n",
    "diagonal_entries_sigma = np.sqrt(np.diag(cov_matrix))\n",
    "c_0_sigma = diagonal_entries_sigma[0]\n",
    "c_1_sigma = diagonal_entries_sigma[1]\n",
    "\n",
    "# Step 8: Use the coefficients to predict y values\n",
    "y_pred = X.dot(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42afdb2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot \n",
    "f, ax = plt.subplots()\n",
    "ax.errorbar(x, y, yerr = y_sigma, c = 'k', fmt = '.')\n",
    "ax.plot(x, y_pred, label = f\"y = ({c_0:.2f} ± {c_0_sigma:.2f}) + ({c_1:.2f} ± {c_1_sigma:.2f}) x\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494b212",
   "metadata": {},
   "source": [
    "### Complexity 2: Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the data and uncertainties\n",
    "x = x_data_full\n",
    "y = y_data_full\n",
    "y_sigma = y_sigma_full\n",
    "\n",
    "# Step 2: Create the design matrix X\n",
    "# We add a column of ones for the bias term (c_0)\n",
    "X = np.column_stack((np.ones(x.shape[0]), x))\n",
    "\n",
    "# Step 3: Create the weights matrix W\n",
    "W = np.diag(1 / y_sigma**2)  # Diagonal matrix of 1/y_sigma^2\n",
    "\n",
    "# Step 4: Compute the weighted normal equation components\n",
    "# X^T W X and X^T W y\n",
    "XT_W_X = np.dot(X.T, np.dot(W, X))\n",
    "XT_W_y = np.dot(X.T, np.dot(W, y))\n",
    "\n",
    "# Step 5: Compute the covariance matrix of the coefficients\n",
    "cov_matrix = np.linalg.inv(XT_W_X)\n",
    "\n",
    "# Step 5: Solve for the coefficients (c_0, c_1)\n",
    "coefficients_out = cov_matrix.dot(XT_W_y)\n",
    "\n",
    "# Step 6: Output the coefficients\n",
    "c_0_out, c_1_out = coefficients_out\n",
    "\n",
    "# Step 7: Exctract coefficient sigma\n",
    "diagonal_entries_sigma = np.sqrt(np.diag(cov_matrix))\n",
    "c_0_sigma_out = diagonal_entries_sigma[0]\n",
    "c_1_sigma_out = diagonal_entries_sigma[1]\n",
    "\n",
    "# Step 8: Use the coefficients to predict y values\n",
    "y_pred_out = X.dot(coefficients_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3fab66",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot \n",
    "f, ax = plt.subplots()\n",
    "ax.errorbar(x_data_simple, y_data_simple, yerr = y_sigma_simple, c = 'k', fmt = '.')\n",
    "ax.errorbar(x, y, yerr = y_sigma, c = 'C0', fmt = '.')\n",
    "ax.plot(x_data_simple, y_pred, label = f\"y = ({c_0:.2f} ± {c_0_sigma:.2f}) + ({c_1:.2f} ± {c_1_sigma:.2f}) x\")\n",
    "ax.plot(x, y_pred_out, label = f\"y = ({c_0_out:.2f} ± {c_0_sigma_out:.2f}) + ({c_1_out:.2f} ± {c_1_sigma_out:.2f}) x\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(fontsize=10)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86265e1",
   "metadata": {},
   "source": [
    "### Complexity 3+: A quadratic function with a slightly different naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f92522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic function\n",
    "\n",
    "# Data points 5 through 20 from Table 1\n",
    "x_data = x_data_simple\n",
    "y_data = y_data_simple\n",
    "y_errors = y_sigma_simple\n",
    "\n",
    "# Quadratic fit (add x^2 column to the design matrix A)\n",
    "A_quad_ex3 = np.vstack([x_data**2, x_data, np.ones(len(x_data))]).T\n",
    "Y_ex3 = y_data\n",
    "C_ex3 = np.diag(y_errors**2)\n",
    "\n",
    "# Solve the least-squares problem using the weighted linear least-squares formula\n",
    "A_T_C_inv_ex3 = np.dot(A_quad_ex3.T, np.linalg.inv(C_ex3))\n",
    "cov_matrix_ex3 = np.linalg.inv(np.dot(A_T_C_inv_ex3, A_quad_ex3))\n",
    "best_fit_params_ex3 = np.dot(cov_matrix_ex3, np.dot(A_T_C_inv_ex3, Y_ex3))\n",
    "\n",
    "# Extract quadratic (q), slope (m), and intercept (b)\n",
    "q_ex3, m_ex3, b_ex3 = best_fit_params_ex3\n",
    "q_uncertainty_ex3 = np.sqrt(cov_matrix_ex3[0, 0])\n",
    "slope_uncertainty_ex3 = np.sqrt(cov_matrix_ex3[1, 1])\n",
    "intercept_uncertainty_ex3 = np.sqrt(cov_matrix_ex3[2, 2])\n",
    "\n",
    "# Print the result\n",
    "print(f\"Quadratic term (q): {q_ex3:.4f} ± {q_uncertainty_ex3:.4f}\")\n",
    "print(f\"Slope (m): {m_ex3:.2f} ± {slope_uncertainty_ex3:.2f}\")\n",
    "print(f\"Intercept (b): {b_ex3:.2f} ± {intercept_uncertainty_ex3:.2f}\")\n",
    "\n",
    "# Generate a smooth set of x values for plotting the quadratic curve\n",
    "x_smooth = np.linspace(min(x_data), max(x_data), 500)\n",
    "\n",
    "# Plot the data and the quadratic fit\n",
    "plt.errorbar(x_data, y_data, yerr=y_errors, fmt='o', label='Data')\n",
    "plt.plot(x_smooth, q_ex3 * x_smooth**2 + m_ex3 * x_smooth + b_ex3, label=f'y = {q_ex3:.4f} ± {q_uncertainty_ex3:.4f}x^2 \\n + {m_ex3:.2f} ± {slope_uncertainty_ex3:.2f}x + {b_ex3:.0f} ± {intercept_uncertainty_ex3:.0f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8413d94",
   "metadata": {},
   "source": [
    "## When should I use what python fitting function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93629ec",
   "metadata": {},
   "source": [
    "### numpy.polyfit\n",
    "\n",
    "Best for: Linear and polynomial fits (up to quadratic and higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Linear fit\n",
    "linear_fit = np.polyfit(x_data, y_data, 1)  # 1 for linear\n",
    "m, b = linear_fit  # slope and intercept\n",
    "\n",
    "# Quadratic fit\n",
    "quadratic_fit = np.polyfit(x_data, y_data, 2)  # 2 for quadratic\n",
    "q, m, b = quadratic_fit  # quadratic, slope, and intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b757e98",
   "metadata": {},
   "source": [
    "### scipy.optimize.curve_fit\n",
    "\n",
    "Best for: Non-linear curve fitting, customizable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define linear model\n",
    "def linear_model(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "# Define quadratic model\n",
    "def quadratic_model(x, q, m, b):\n",
    "    return q * x**2 + m * x + b\n",
    "\n",
    "# curve_fit will provide you with optimal parameters and a parameter covariance matrix.\n",
    "# To extract the standard deviation of the fitted parameters, you can use np.sqrt(np.diag())\n",
    "# to extract the square root of the diagonal entries\n",
    "\n",
    "# Linear fit with uncertainties\n",
    "popt_linear, pcov_linear = curve_fit(linear_model, x_data, y_data, sigma=y_errors, absolute_sigma=True)\n",
    "m_linear, b_linear = popt_linear\n",
    "perr_linear = np.sqrt(np.diag(pcov_linear))\n",
    "\n",
    "# Quadratic fit with uncertainties\n",
    "popt_quad, pcov_quad = curve_fit(quadratic_model, x_data, y_data, sigma=y_errors, absolute_sigma=True)\n",
    "q_quad, m_quad, b_quad = popt_quad\n",
    "perr_quad = np.sqrt(np.diag(pcov_quad))\n",
    "\n",
    "# Print results with uncertainties\n",
    "print(f\"Linear Fit: y = {m_linear:.2f}x + {b_linear:.2f}\")\n",
    "print(f\"Linear Fit uncertainties: m = {perr_linear[0]:.2f}, b = {perr_linear[1]:.2f}\")\n",
    "print(f\"Quadratic Fit: y = {q_quad:.4f}x^2 + {m_quad:.2f}x + {b_quad:.2f}\")\n",
    "print(f\"Quadratic Fit uncertainties: q = {perr_quad[0]:.4f}, m = {perr_quad[1]:.2f}, b = {perr_quad[2]:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "x_smooth = np.linspace(min(x_data), max(x_data), 500)\n",
    "plt.errorbar(x_data, y_data, yerr=y_errors, fmt='o', label='Data')\n",
    "\n",
    "# Plot linear and quadratic fits\n",
    "plt.plot(x_smooth, linear_model(x_smooth, *popt_linear), label=f'Linear Fit: y = {m_linear:.2f}x + {b_linear:.2f}')\n",
    "plt.plot(x_smooth, quadratic_model(x_smooth, *popt_quad), label=f'Quadratic Fit: \\n y = {q_quad:.4f}x^2 + {m_quad:.2f}x + {b_quad:.2f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad855fd",
   "metadata": {},
   "source": [
    "### statsmodels.api as sm\n",
    "\n",
    "Best for: Regression analysis with robust statistical output, linear and polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Linear model\n",
    "x_with_intercept = sm.add_constant(x_data)  # Adds the intercept term\n",
    "linear_model = sm.OLS(y_data, x_with_intercept).fit()\n",
    "\n",
    "# Print linear model summary (with statistical info)\n",
    "print(linear_model.summary())\n",
    "\n",
    "# Quadratic model\n",
    "x_data_quad = np.column_stack((x_data**2, x_data))\n",
    "x_data_quad_with_intercept = sm.add_constant(x_data_quad)\n",
    "quadratic_model = sm.OLS(y_data, x_data_quad_with_intercept).fit()\n",
    "\n",
    "# Print quadratic model summary\n",
    "print(quadratic_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432158c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# And with uncertainties:\n",
    "\n",
    "# Weights (1/variance)\n",
    "weights = 1 / y_errors**2\n",
    "\n",
    "# Linear fit with weights\n",
    "x_data_linear = sm.add_constant(x_data)\n",
    "model_linear = sm.WLS(y_data, x_data_linear, weights=weights)\n",
    "result_linear = model_linear.fit()\n",
    "\n",
    "# Quadratic fit with weights\n",
    "x_data_quad = np.column_stack((x_data, x_data**2))\n",
    "x_data_quad = sm.add_constant(x_data_quad)\n",
    "model_quad = sm.WLS(y_data, x_data_quad, weights=weights)\n",
    "result_quad = model_quad.fit()\n",
    "\n",
    "# Print summaries\n",
    "print(result_linear.summary())\n",
    "print(result_quad.summary())\n",
    "\n",
    "# Plot data\n",
    "x_smooth = np.linspace(min(x_data), max(x_data), 500)\n",
    "plt.errorbar(x_data, y_data, yerr=y_errors, fmt='o', label='Data')\n",
    "\n",
    "# Plot linear fit\n",
    "linear_fit = result_linear.params[0] + result_linear.params[1] * x_smooth\n",
    "plt.plot(x_smooth, linear_fit, label='Linear Fit')\n",
    "\n",
    "# Plot quadratic fit\n",
    "quad_fit = result_quad.params[0] + result_quad.params[1] * x_smooth + result_quad.params[2] * x_smooth**2\n",
    "plt.plot(x_smooth, quad_fit, label='Quadratic Fit')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013c0e2",
   "metadata": {},
   "source": [
    "## What if I also have uncertainties on my x-data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd1b29",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ee401",
   "metadata": {},
   "source": [
    "#### Total Uncertainty ($\\sigma_{\\text{total}}$) in $y_i$\n",
    "\n",
    "The **total uncertainty** in $y_i$, denoted as $\\sigma_{\\text{total}, i}$, arises from two sources:\n",
    "1. **Measurement error in $y_i$**, given by $\\sigma_{y_i}$.\n",
    "2. **Measurement error in $x_i$**, which propagates through the model and contributes to the uncertainty in $y_i$.\n",
    "\n",
    "To compute the total uncertainty, we must propagate the uncertainty in $x_i$ through the quadratic model $y = q x^2 + m x + b$.\n",
    "\n",
    "#### Propagating the Uncertainty in $x_i$\n",
    "\n",
    "To propagate the uncertainty in $x_i$ to the predicted value of $y_i$, we use the following general rule for **error propagation**:\n",
    "\n",
    "If $y$ depends on $x$ through some function $y = f(x)$, the uncertainty in $y$ due to the uncertainty in $x$ is given by:\n",
    "$$\n",
    "\\sigma_{y, \\text{prop}}^2 = \\left( \\frac{\\partial f}{\\partial x} \\right)^2 \\sigma_x^2\n",
    "$$\n",
    "Where:\n",
    "- $\\frac{\\partial f}{\\partial x}$ is the derivative of the function $f(x)$ with respect to $x$,\n",
    "- $\\sigma_x$ is the uncertainty in $x$,\n",
    "- $\\sigma_{y, \\text{prop}}$ is the propagated uncertainty in $y$.\n",
    "\n",
    "Our model is:\n",
    "$$\n",
    "y = q x^2 + m x + b\n",
    "$$\n",
    "The derivative of $y$ with respect to $x$ is:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = 2 q x + m\n",
    "$$\n",
    "\n",
    "Therefore, the uncertainty in $y_i$ due to the uncertainty in $x_i$ is:\n",
    "$$\n",
    "\\sigma_{y, \\text{prop}, i}^2 = \\left( 2 q x_i + m \\right)^2 \\sigma_{x_i}^2\n",
    "$$\n",
    "Where:\n",
    "- $\\sigma_{x_i}$ is the uncertainty in the measurement of $x_i$,\n",
    "- $2 q x_i + m$ is the rate of change of $y$ with respect to $x$.\n",
    "\n",
    "#### Total Uncertainty in $y_i$\n",
    "\n",
    "The total uncertainty in $y_i$, denoted as $\\sigma_{\\text{total}, i}^2$, combines the measurement uncertainty in $y_i$ and the propagated uncertainty from $x_i$. These uncertainties are independent, so we sum them in quadrature (sum of squares):\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{total}, i}^2 = \\sigma_{y_i}^2 + \\left( 2 q x_i + m \\right)^2 \\sigma_{x_i}^2\n",
    "$$\n",
    "\n",
    "This is the total variance for each data point $i$, which incorporates both the uncertainty in $y$ and the uncertainty in $x$.\n",
    "\n",
    "#### Likelihood Function\n",
    "\n",
    "When fitting a model to data using a Gaussian likelihood, the likelihood function typically assumes that the residuals (differences between the observed and predicted values) are Gaussian-distributed. For a given data point $i$, the likelihood contribution is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_i = \\frac{1}{\\sqrt{2 \\pi \\sigma_{\\text{total}, i}^2}} \\exp\\left( -\\frac{(y_i - y_{\\text{model}, i})^2}{2 \\sigma_{\\text{total}, i}^2} \\right)\n",
    "$$\n",
    "Where:\n",
    "- $y_i$ is the observed $y$-value for data point $i$,\n",
    "- $y_{\\text{model}, i}$ is the predicted $y$-value from the quadratic model for data point $i$,\n",
    "- $\\sigma_{\\text{total}, i}^2$ is the total variance, including both $\\sigma_{y_i}^2$ and the propagated uncertainty from $x_i$.\n",
    "\n",
    "#### Log-Likelihood\n",
    "\n",
    "In practice, we typically work with the **log-likelihood** (which simplifies the math by turning products of likelihoods into sums):\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L} = -\\frac{1}{2} \\sum_i \\left[ \\frac{(y_i - y_{\\text{model}, i})^2}{\\sigma_{\\text{total}, i}^2} + \\log \\left( 2 \\pi \\sigma_{\\text{total}, i}^2 \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_{\\text{model}, i} = q x_i^2 + m x_i + b$ is the predicted $y$-value from the quadratic model,\n",
    "- $\\sigma_{\\text{total}, i}^2 = \\sigma_{y_i}^2 + \\left( 2 q x_i + m \\right)^2 \\sigma_{x_i}^2$ is the total variance for data point $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea25be",
   "metadata": {},
   "source": [
    "**And in practice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dca6b6",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "To find the best-fit parameters \\(q\\), \\(m\\), and \\(b\\), we can **maximize the log-likelihood** using an optimization routine like `scipy.optimize.minimize`. We will minimize the **negative log-likelihood** (since minimizing a negative function is equivalent to maximizing the positive function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Data (replace with your actual data)\n",
    "x_data = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157, 131, 166, 160, 186, 125, 218, 146])\n",
    "y_data = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344])\n",
    "x_errors = np.array([9, 4, 11, 7, 5, 9, 4, 4, 11, 7, 5, 5, 9, 8, 6, 5])\n",
    "y_errors = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22])\n",
    "\n",
    "# Quadratic model\n",
    "def quadratic_model(params, x):\n",
    "    q, m, b = params\n",
    "    return q * x**2 + m * x + b\n",
    "\n",
    "# Total uncertainty including both y-errors and propagated x-errors\n",
    "def total_uncertainty(params, x, xerr, yerr):\n",
    "    q, m, b = params\n",
    "    return np.sqrt(yerr**2 + (2 * q * x + m)**2 * xerr**2)\n",
    "\n",
    "# Negative log-likelihood function\n",
    "def neg_log_likelihood(params, x, y, xerr, yerr):\n",
    "    model = quadratic_model(params, x)\n",
    "    sigma_total = total_uncertainty(params, x, xerr, yerr)\n",
    "    log_likelihood = -0.5 * np.sum(((y - model) / sigma_total)**2 + np.log(2 * np.pi * sigma_total**2))\n",
    "    return -log_likelihood  # We return the negative log-likelihood to minimize it\n",
    "\n",
    "# Initial guess for the parameters (q, m, b)\n",
    "initial_guess = [0.001, 1, 100]\n",
    "\n",
    "# Perform the optimization (MLE)\n",
    "result = minimize(neg_log_likelihood, initial_guess, args=(x_data, y_data, x_errors, y_errors))\n",
    "\n",
    "# Extract the best-fit parameters\n",
    "best_q, best_m, best_b = result.x\n",
    "\n",
    "# Estimate the uncertainties: The square root of the diagonal of the covariance matrix\n",
    "cov_matrix = result.hess_inv  # This is the estimated covariance matrix\n",
    "uncertainties = np.sqrt(np.diag(cov_matrix))\n",
    "\n",
    "# Extract the uncertainties\n",
    "q_uncertainty, m_uncertainty, b_uncertainty = uncertainties\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best-fit parameters from MLE:\")\n",
    "print(\"q: \",round_to_significant_digits(best_q, q_uncertainty))\n",
    "print(\"m: \",round_to_significant_digits(best_m, m_uncertainty))\n",
    "print(\"b: \",round_to_significant_digits(best_b, b_uncertainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06a391",
   "metadata": {},
   "source": [
    "### scipy.odr: Orthogonal Distance Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d7736",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# scipy.odr \n",
    "from scipy.odr import ODR, Model, RealData\n",
    "\n",
    "# Data (example data, replace with your actual data)\n",
    "x_data = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157, 131, 166, 160, 186, 125, 218, 146])\n",
    "y_data = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344])\n",
    "x_errors = np.array([9, 4, 11, 7, 5, 9, 4, 4, 11, 7, 5, 5, 9, 8, 6, 5])\n",
    "y_errors = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22])\n",
    "\n",
    "# Prepare the real data with uncertainties in both x and y\n",
    "data = RealData(x_data, y_data, sx=x_errors, sy=y_errors)\n",
    "\n",
    "# Define the quadratic model\n",
    "def quadratic_model(parameters, x):\n",
    "    q, m, b = parameters\n",
    "    return q * x**2 + m * x + b\n",
    "\n",
    "# Create a Model object based on the quadratic function\n",
    "model = Model(quadratic_model)\n",
    "\n",
    "# Set up ODR with the initial guess for the parameters (q, m, b)\n",
    "odr = ODR(data, model, beta0=[0.001, 1, 100])\n",
    "\n",
    "# Run the ODR fitting process\n",
    "output = odr.run()\n",
    "\n",
    "# Extract the fitted parameters\n",
    "q_odr, m_odr, b_odr = output.beta\n",
    "q_odr_err, m_odr_err, b_odr_err = output.sd_beta  # Standard deviations (uncertainties) of the parameters\n",
    "\n",
    "# Print the results\n",
    "print(f\"ODR Fit: y = {q_odr:.4f}x^2 + {m_odr:.2f}x + {b_odr:.2f}\")\n",
    "print(f\"Uncertainties: q_err = {q_odr_err:.4f}, m_err = {m_odr_err:.2f}, b_err = {b_odr_err:.2f}\")\n",
    "\n",
    "# Plot the result\n",
    "x_smooth = np.linspace(min(x_data), max(x_data), 500)\n",
    "y_fit = quadratic_model([q_odr, m_odr, b_odr], x_smooth)\n",
    "\n",
    "# Plot data with error bars\n",
    "plt.errorbar(x_data, y_data, xerr=x_errors, yerr=y_errors, fmt='o', label='Data')\n",
    "\n",
    "# Plot the best fit quadratic curve\n",
    "plt.plot(x_smooth, q_odr * x_smooth**2 + m_odr * x_smooth + b_odr, label=f'ODR Fit: y = {q_odr:.4f}x^2 + {m_odr:.2f}x + {b_odr:.2f}', color='blue')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed294f82",
   "metadata": {},
   "source": [
    "### Sneak Preview: Bayesian Fitting with a flat prior\n",
    "\n",
    "You will discuss this in much more detail with Yuxiang in week 12.\n",
    "\n",
    "But because it is soooooooo useful, I am already giving you a very basic example here.\n",
    "\n",
    "This example is so simple, it is not even capitalizing the true power of Bayesian fitting: How you can fit better if you have additional information! This is usually done with a \"prior\".\n",
    "\n",
    "In our example, we will simply assume that all coefficients that are numbers are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514790a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "# Data (example data, replace with your actual data)\n",
    "x_data = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157, 131, 166, 160, 186, 125, 218, 146])\n",
    "y_data = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344])\n",
    "x_errors = np.array([9, 4, 11, 7, 5, 9, 4, 4, 11, 7, 5, 5, 9, 8, 6, 5])\n",
    "y_errors = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22])\n",
    "\n",
    "# Define the log-likelihood function that accounts for uncertainties in both x and y\n",
    "def log_likelihood(parameters, x, y, xerr, yerr):\n",
    "    q, m, b = parameters\n",
    "    model = quadratic_model(parameters, x)\n",
    "    sigma2 = yerr**2 + (2 * q * x + m)**2 * xerr**2  # Propagate x and y uncertainties\n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))\n",
    "\n",
    "# Define the log-prior function. Here we simply say: our coefficients have to be numbers.\n",
    "def log_prior(parameters):\n",
    "    q, m, b = parameters\n",
    "    if abs(q) >= 0 and abs(m) >= 0 and abs(b) >= 0:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "# Define the log-probability function\n",
    "def log_probability(parameters, x, y, xerr, yerr):\n",
    "    lp = log_prior(parameters)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(parameters, x, y, xerr, yerr)\n",
    "\n",
    "# Initial guess for the parameters (q, m, b)\n",
    "initial = np.array([1, 1, 1])\n",
    "\n",
    "# Set up the MCMC sampler\n",
    "nwalkers = 32\n",
    "ndim = len(initial)\n",
    "pos = initial + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "# Set up and run the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(x_data, y_data, x_errors, y_errors))\n",
    "sampler.run_mcmc(pos, 5000)\n",
    "\n",
    "# Get the samples from the posterior\n",
    "samples = sampler.get_chain(discard=100, thin=10, flat=True)\n",
    "\n",
    "# Get the best fit parameters by extracting the 16th, 50th, and 84th percentiles\n",
    "q_mcmc_percentiles = np.percentile(samples[:, 0], [16, 50, 84])\n",
    "m_mcmc_percentiles = np.percentile(samples[:, 1], [16, 50, 84])\n",
    "b_mcmc_percentiles = np.percentile(samples[:, 2], [16, 50, 84])\n",
    "\n",
    "# Define a function to print the percentiles in the desired LaTeX format\n",
    "def print_percentiles_latex(parameter_name, percentiles):\n",
    "    median = percentiles[1]\n",
    "    lower_error = median - percentiles[0]\n",
    "    upper_error = percentiles[2] - median\n",
    "    print(f\"${parameter_name} = {median:.4f}_{{-{lower_error:.4f}}}^{{+{upper_error:.4f}}}$\")\n",
    "\n",
    "# Print the percentiles for q, m, and b in LaTeX format\n",
    "print(\"Quadratic MCMC fit:\")\n",
    "print_percentiles_latex(\"q\", q_mcmc_percentiles)\n",
    "print_percentiles_latex(\"m\", m_mcmc_percentiles)\n",
    "print_percentiles_latex(\"b\", b_mcmc_percentiles)\n",
    "\n",
    "# Plot the results\n",
    "x_smooth = np.linspace(min(x_data), max(x_data), 500)\n",
    "\n",
    "# Plot data with error bars\n",
    "plt.errorbar(x_data, y_data, xerr=x_errors, yerr=y_errors, fmt='o', label='Data')\n",
    "\n",
    "# Plot the best fit quadratic curve\n",
    "plt.plot(x_smooth, quadratic_model([q_mcmc_percentiles[1], m_mcmc_percentiles[1], b_mcmc_percentiles[1]], x_smooth), label=f'Quadratic Fit: y = {q_mcmc_percentiles[1]:.4f}x^2 + {m_mcmc_percentiles[1]:.2f}x + {b_mcmc_percentiles[1]:.2f}', color='green')\n",
    "plt.plot(x_smooth, q_odr * x_smooth**2 + m_odr * x_smooth + b_odr, label=f'ODR Fit: y = {q_odr:.4f}x^2 + {m_odr:.2f}x + {b_odr:.2f}', color='blue')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(fontsize=10, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa76cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "# Create the corner plot\n",
    "fig = corner.corner(samples, labels=[r\"$q$\", r\"$m$\", r\"$b$\"], \n",
    "                    truths=[q_odr, m_odr, b_odr],\n",
    "                    truth_color='red', show_titles=True, title_fmt=\".4f\")\n",
    "\n",
    "# Extract the axes for adding ODR uncertainties\n",
    "axes = np.array(fig.axes).reshape((3, 3))\n",
    "\n",
    "# Overlay ODR best-fit values and uncertainties\n",
    "for i, (value, err) in enumerate(zip([q_odr, m_odr, b_odr], [q_odr_err, m_odr_err, b_odr_err])):\n",
    "    ax = axes[i, i]\n",
    "    ax.axvline(value, color=\"red\", linestyle=\"--\")\n",
    "    ax.axvline(value + err, color=\"red\", linestyle=\":\")\n",
    "    ax.axvline(value - err, color=\"red\", linestyle=\":\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa133c4",
   "metadata": {},
   "source": [
    "## Jackknife and Bootstrap Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae073ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Example data from Exercise 2\n",
    "x_data = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157, 131, 166, 160, 186, 125, 218, 146])\n",
    "y_data = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344])\n",
    "y_errors = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22])\n",
    "\n",
    "# Define the linear model\n",
    "def linear_model(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "# Step 1: Standard Fit\n",
    "popt, pcov = curve_fit(linear_model, x_data, y_data, sigma=y_errors)\n",
    "m_standard, b_standard = popt\n",
    "sigma_m_standard = np.sqrt(pcov[0, 0])  # Uncertainty in slope m from the fit\n",
    "\n",
    "print(f\"Standard slope (m): {m_standard}\")\n",
    "print(f\"Standard uncertainty in slope (sigma_m^2): {sigma_m_standard**2}\")\n",
    "\n",
    "# Step 2: Jackknife Resampling\n",
    "n = len(x_data)\n",
    "jackknife_slopes = []\n",
    "\n",
    "for i in range(n):\n",
    "    # Remove the i-th data point\n",
    "    x_jackknife = np.delete(x_data, i)\n",
    "    y_jackknife = np.delete(y_data, i)\n",
    "    y_errors_jackknife = np.delete(y_errors, i)\n",
    "    \n",
    "    # Fit the model\n",
    "    popt_jackknife, _ = curve_fit(linear_model, x_jackknife, y_jackknife, sigma=y_errors_jackknife)\n",
    "    m_jackknife, _ = popt_jackknife\n",
    "    jackknife_slopes.append(m_jackknife)\n",
    "\n",
    "# Jackknife estimate of uncertainty\n",
    "jackknife_slopes = np.array(jackknife_slopes)\n",
    "m_jackknife_mean = np.mean(jackknife_slopes)\n",
    "sigma_m_jackknife = np.sqrt((n - 1) * np.mean((jackknife_slopes - m_jackknife_mean) ** 2))\n",
    "\n",
    "print(f\"Jackknife uncertainty in slope (sigma_m^2): {sigma_m_jackknife**2}\")\n",
    "\n",
    "# Step 3: Bootstrap Resampling (20 trials)\n",
    "n_trials = 20\n",
    "bootstrap_slopes = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    # Resample the data with replacement\n",
    "    x_bootstrap, y_bootstrap, y_errors_bootstrap = resample(x_data, y_data, y_errors)\n",
    "    \n",
    "    # Fit the model\n",
    "    popt_bootstrap, _ = curve_fit(linear_model, x_bootstrap, y_bootstrap, sigma=y_errors_bootstrap)\n",
    "    m_bootstrap, _ = popt_bootstrap\n",
    "    bootstrap_slopes.append(m_bootstrap)\n",
    "\n",
    "# Bootstrap estimate of uncertainty\n",
    "bootstrap_slopes = np.array(bootstrap_slopes)\n",
    "sigma_m_bootstrap = np.std(bootstrap_slopes)\n",
    "\n",
    "print(f\"Bootstrap uncertainty in slope (sigma_m^2): {sigma_m_bootstrap**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be53b73",
   "metadata": {},
   "source": [
    "## A closer look at uncertainties in likelihood functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023aa153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(params, x, y, sigma_x, sigma_y, rho_xy, model='linear'):\n",
    "    \"\"\"\n",
    "    Optimized log-likelihood for a linear or quadratic model, \n",
    "    including uncertainties in both x and y, and correlations between x and y uncertainties.\n",
    "    \n",
    "    Parameters:\n",
    "    - params: parameters of the model (e.g., [q, m, b] for quadratic, or [m, b] for linear)\n",
    "    - x: array of independent variables (x-coordinates of data points)\n",
    "    - y: array of dependent variables (y-coordinates of data points)\n",
    "    - sigma_x: array of uncertainties in x\n",
    "    - sigma_y: array of uncertainties in y\n",
    "    - rho_xy: array of correlation coefficients between x and y uncertainties\n",
    "    - model: 'linear' or 'quadratic' (default is 'linear')\n",
    "    \n",
    "    Returns:\n",
    "    - log_likelihood: the log-likelihood value\n",
    "    \"\"\"\n",
    "    # Vectorized model predictions and slopes\n",
    "    if model == 'linear':\n",
    "        m, b = params\n",
    "        y_model = m * x + b\n",
    "        slope = np.full_like(x, m)  # Constant slope for linear model\n",
    "    \n",
    "    elif model == 'quadratic':\n",
    "        q, m, b = params\n",
    "        y_model = q * x**2 + m * x + b\n",
    "        slope = 2 * q * x + m  # Slope varies with x for quadratic model\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Model type must be 'linear' or 'quadratic'\")\n",
    "    \n",
    "    # Compute the residuals (differences between actual and predicted y-values)\n",
    "    delta = y - y_model\n",
    "\n",
    "    # Precompute terms for the covariance matrix projection\n",
    "    v_perp_x = -(slope) / np.sqrt(slope**2 + 1)  # Perpendicular direction for x-component\n",
    "    v_perp_y = 1 / np.sqrt(slope**2 + 1)  # Perpendicular direction for y-component\n",
    "    \n",
    "    # Covariance matrix diagonal and off-diagonal terms\n",
    "    sigma_x2 = sigma_x**2\n",
    "    sigma_y2 = sigma_y**2\n",
    "    cov_xy = rho_xy * sigma_x * sigma_y  # Off-diagonal terms in the covariance matrix\n",
    "    \n",
    "    # Total uncertainty in the perpendicular direction (vectorized)\n",
    "    sigma_perp2 = (v_perp_x**2 * sigma_x2 + v_perp_y**2 * sigma_y2 +\n",
    "                   2 * v_perp_x * v_perp_y * cov_xy)\n",
    "\n",
    "    # Compute the log-likelihood in a vectorized fashion\n",
    "    log_likelihood = -0.5 * np.sum((delta**2 / sigma_perp2) + np.log(2 * np.pi * sigma_perp2))\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Data for points 5 through 20 from Table 1\n",
    "x_data = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157, 131, 166, 160, 186, 125, 218, 146])\n",
    "y_data = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344])\n",
    "sigma_x = np.array([5, 9, 4, 4, 11, 7, 5, 5, 11, 7, 5, 5, 9, 8, 6, 5])\n",
    "sigma_y = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22])\n",
    "rho_xy = np.array([-0.33, 0.67, -0.02, -0.05, -0.84, -0.69, 0.30, -0.46, -0.03, 0.50, 0.73, -0.52, 0.90, 0.40, -0.78, -0.56])\n",
    "\n",
    "# Covariance matrix assuming uncorrelated errors\n",
    "def covariance_matrix(sigma_x, sigma_y, rho_xy):\n",
    "    return np.array([[sigma_x**2, rho_xy * sigma_x * sigma_y],\n",
    "                     [rho_xy * sigma_x * sigma_y, sigma_y**2]])\n",
    "\n",
    "model = 'quadratic' # can be linear or quadratic\n",
    "\n",
    "if model == 'linear':\n",
    "    # Initial guess for slope (m) and intercept (b)\n",
    "    initial_guess = [1, 100]\n",
    "elif model == 'quadratic':\n",
    "    # Initial guess for slope (m) and intercept (b)\n",
    "    initial_guess = [1, 2, 70]\n",
    "else:\n",
    "    raise ValueError(\"Model type must be 'linear' or 'quadratic'\")\n",
    "\n",
    "def neg_log_likelihood(params, x, y, sigma_x, sigma_y, rho_xy, model=model):\n",
    "    return -log_likelihood(params, x, y, sigma_x, sigma_y, rho_xy, model=model)\n",
    "\n",
    "# Perform minimization to find the best-fit parameters\n",
    "result = minimize(neg_log_likelihood, initial_guess, args=(x_data, y_data, sigma_x, sigma_y, rho_xy))\n",
    "\n",
    "if model == 'linear':\n",
    "    # Extract the best-fit slope and intercept\n",
    "    best_m, best_b = result.x\n",
    "    \n",
    "    # Plot best-fit line\n",
    "    x_fit = np.linspace(np.min(x_data), np.max(x_data))\n",
    "    y_fit = best_m * x_fit + best_b\n",
    "\n",
    "elif model == 'quadratic':\n",
    "    # Extract the best-fit slope and intercept\n",
    "    best_q, best_m, best_b = result.x\n",
    "    \n",
    "    # Plot best-fit line\n",
    "    x_fit = np.linspace(np.min(x_data), np.max(x_data))\n",
    "    y_fit = best_q * x_fit**2 + best_m * x_fit + best_b\n",
    "\n",
    "# Plot the data with error ellipses\n",
    "def plot_error_ellipses(x_data, y_data, sigma_x, sigma_y, rho_xy):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot data points with ellipses\n",
    "    for i in range(len(x_data)):\n",
    "        cov = covariance_matrix(sigma_x[i], sigma_y[i], rho_xy[i])\n",
    "        lambda_, v = np.linalg.eig(cov)  # Eigenvalues and eigenvectors\n",
    "        lambda_ = np.sqrt(lambda_)  # Square roots of eigenvalues give axis lengths\n",
    "        \n",
    "        # Compute the angle of the ellipse (from the eigenvector corresponding to the largest eigenvalue)\n",
    "        angle = np.arctan2(v[1, 0], v[0, 0]) * 180.0 / np.pi\n",
    "        \n",
    "        ell = plt.matplotlib.patches.Ellipse(xy=(x_data[i], y_data[i]),\n",
    "                                             width=lambda_[0]*2, height=lambda_[1]*2,\n",
    "                                             angle=angle, edgecolor='k', facecolor='none')\n",
    "        ax.add_patch(ell)\n",
    "    \n",
    "    if model == 'linear':\n",
    "        ax.plot(x_fit, y_fit, color='C3', label=f'Best fit: y = {best_m:.2f}x + {best_b:.2f}')\n",
    "    elif model == 'quadratic':\n",
    "        ax.plot(x_fit, y_fit, color='C3', label=f'Best fit: y = {best_q:.4f}x^2 + {best_m:.2f}x + {best_b:.0f}')\n",
    "        \n",
    "    # Scatter plot of data points\n",
    "    ax.scatter(x_data, y_data, color='k', label='Data')\n",
    "    \n",
    "    # Labels and legend\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    plt.title('Best-fit Line with Error Ellipses')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the data with error ellipses and best-fit line\n",
    "plot_error_ellipses(x_data, y_data, sigma_x, sigma_y, rho_xy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58787ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "\n",
    "# Data from Table 1 (points 5 to 20)\n",
    "x_data = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157, 131, 166, 160, 186, 125, 218, 146])\n",
    "y_data = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344])\n",
    "sigma_x = np.array([5, 9, 4, 4, 11, 7, 5, 5, 11, 7, 5, 5, 9, 8, 6, 5])\n",
    "sigma_y = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52, 16, 34, 31, 42, 26, 16, 22])\n",
    "\n",
    "# Covariance matrix assuming uncorrelated errors\n",
    "def covariance_matrix(sigma_x, sigma_y):\n",
    "    return np.array([[sigma_x**2, 0],\n",
    "                     [0, sigma_y**2]])\n",
    "\n",
    "# Quadratic model\n",
    "def quadratic_model(x, q, m, b):\n",
    "    return q * x**2 + m * x + b\n",
    "\n",
    "# Orthogonal distance calculation for a quadratic function\n",
    "def orthogonal_distance_quadratic(params, x, y, sigma_x, sigma_y):\n",
    "    q, m, b = params\n",
    "    y_model = quadratic_model(x, q, m, b)  # Compute the y value from the quadratic model\n",
    "    delta = y - y_model  # Distance between the data point and the curve\n",
    "    S = covariance_matrix(sigma_x, sigma_y)  # Covariance matrix (uncorrelated errors)\n",
    "    # Calculate perpendicular distance (same unit vector approach)\n",
    "    v_perp = np.array([2 * q * x + m, -1]) / np.sqrt((2 * q * x + m)**2 + 1)  # Unit vector orthogonal to the curve\n",
    "    sigma_perp = np.dot(np.dot(v_perp.T, S), v_perp)  # Projected uncertainty\n",
    "    return delta**2 / sigma_perp  # Normalized distance\n",
    "\n",
    "# Log-likelihood function for MCMC (based on orthogonal distance)\n",
    "def log_likelihood(params, x, y, sigma_x, sigma_y):\n",
    "    residuals = [orthogonal_distance_quadratic(params, x[i], y[i], sigma_x[i], sigma_y[i]) for i in range(len(x))]\n",
    "    return -0.5 * np.sum(residuals)\n",
    "\n",
    "# Define the log-prior function (uniform priors)\n",
    "def log_prior(params):\n",
    "    q, m, b = params\n",
    "    if abs(q) >= 0 and abs(m) >= 0 and abs(b) >= 0:  # Priors on q, m, and b\n",
    "        return 0.0\n",
    "    return -np.inf  # Return -inf outside the bounds\n",
    "\n",
    "# Define the log-posterior function (log-prior + log-likelihood)\n",
    "def log_posterior(params, x, y, sigma_x, sigma_y):\n",
    "    lp = log_prior(params)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(params, x, y, sigma_x, sigma_y)\n",
    "\n",
    "# Initial guess for q, m, and b\n",
    "initial_guess = [0.00, 2., 50]\n",
    "\n",
    "# Set up the MCMC sampler\n",
    "ndim = 3  # Number of parameters (q, m, b)\n",
    "nwalkers = 50  # Number of MCMC walkers\n",
    "nsteps = 500  # Number of MCMC steps per walker\n",
    "pos = initial_guess + 1e-4 * np.random.randn(nwalkers, ndim)  # Initial positions of walkers\n",
    "\n",
    "# Set up the MCMC sampler using emcee\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(x_data, y_data, sigma_x, sigma_y))\n",
    "\n",
    "# Run the MCMC sampler\n",
    "sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "# Extract the samples\n",
    "samples = sampler.get_chain(discard=100, thin=15, flat=True)\n",
    "\n",
    "# Plot the MCMC chains\n",
    "import corner\n",
    "\n",
    "# Corner plot to visualize the posterior distributions\n",
    "fig = corner.corner(samples, labels=[\"q\", \"m\", \"b\"], truths=initial_guess)\n",
    "plt.show()\n",
    "\n",
    "# Extract the best-fit parameters (median of posterior)\n",
    "q_mcmc, m_mcmc, b_mcmc = np.median(samples, axis=0)\n",
    "print(f\"Best-fit quadratic coefficient (q) from MCMC: {q_mcmc}\")\n",
    "print(f\"Best-fit slope (m) from MCMC: {m_mcmc}\")\n",
    "print(f\"Best-fit intercept (b) from MCMC: {b_mcmc}\")\n",
    "\n",
    "# Plot the data with error bars and the MCMC best-fit line\n",
    "def plot_mcmc_fit(x_data, y_data, sigma_x, sigma_y, q_best, m_best, b_best):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot error bars\n",
    "    ax.errorbar(x_data, y_data, xerr=sigma_x, yerr=sigma_y, fmt='o', label='Data', capsize=3)\n",
    "\n",
    "    # Plot best-fit quadratic curve\n",
    "    x_fit = np.linspace(min(x_data), max(x_data), 100)\n",
    "    y_fit = quadratic_model(x_fit, q_best, m_best, b_best)\n",
    "    ax.plot(x_fit, y_fit, color='green', label=f'Best fit (MCMC):\\n y = {q_best:.4f}x^2 + {m_best:.2f}x + {b_best:.2f}')\n",
    "    \n",
    "    # Scatter plot of data points\n",
    "    ax.scatter(x_data, y_data, color='blue', label='Data')\n",
    "    \n",
    "    # Labels and legend\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "    plt.title('Best-fit Quadratic Curve with Error Bars (MCMC, Orthogonal Distance)')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the MCMC fit\n",
    "plot_mcmc_fit(x_data, y_data, sigma_x, sigma_y, q_mcmc, m_mcmc, b_mcmc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2db00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
